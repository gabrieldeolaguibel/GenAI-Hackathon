{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a30b6137-17ef-4f2d-8858-c49247077fca",
   "metadata": {},
   "source": [
    "## Welcome to this AI Workshop / Bootcamp\n",
    "\n",
    "#### This tutorial is written by Ikhlaq Sidhu\n",
    "#### The learning objective to use be able to use the OpenAI API, and to provide examples that can be used for different types of applications and products and services.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f7a629d-84f0-49cc-91fa-0d5c8a447f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for reference, see https://platform.openai.com/docs/api-reference/introduction\n",
    "\n",
    "# before this code will work, you must do the following at a shell prompt:\n",
    "# pip install openai\n",
    "# from inside the notebook, try the following. \n",
    "#!pip install openai --upgrade --force-reinstall\n",
    "#!pip install openai\n",
    "# Note the '!' is a magic command to allow you to do it from the notebook. Don't use it from the shell\n",
    "\n",
    "#if you plan to use nodejs, then this will also be needed:\n",
    "#npm install openai@^4.0.0\n",
    "\n",
    "# this code uses my account for authentication.  If you develop code, \n",
    "# you should sign up for your own account at https://platform.openai.com/\n",
    "# then you can use a credit card and set limits (ie x euro or dollars per month maximum)\n",
    "# when you do this , you will get an api key and an organization key, \n",
    "# The organization key is in the settings menu area. The API key is in the API Keys menu area.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d85de4a3-bd22-4384-b00e-ff8731ab069f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are the squares of integers 0 to 3:\n",
      "0 0\n",
      "1 1\n",
      "2 4\n",
      "3 9\n"
     ]
    }
   ],
   "source": [
    "# this is actual python code - you can try any commands here\n",
    "print ('What are the squares of integers 0 to 3:')\n",
    "for i in range(0,4): print (i,i**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54b65f29-5f3f-481e-bb7c-c8ce874c7e2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'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'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# These keys is needed to \"login\" and \"authenticate\"\n",
    "# Read keys so they are not written in the code\n",
    "filename = 'api_key.txt'\n",
    "with open(filename, 'r') as file: \n",
    "    api_key = file.read().strip() #apikey\n",
    "    \n",
    "filename = 'api_org.txt'\n",
    "with open(filename, 'r') as file:\n",
    "    api_key2 = file.read().strip() #organization\n",
    "\n",
    "api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f172e2af-5102-4fa5-896d-de7f2840d471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will create a client object using the OpenAI api to make requests\n",
    "# For this we need to use the keys so that is is authenticated and charges can be made\n",
    "# This is the more recent APY - for 'OpenAI', not 'openai'\n",
    "\n",
    "import os\n",
    "from openai import OpenAI\n",
    "myclient = OpenAI(api_key = api_key, organization=api_key2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffcd38a-d4ea-4b5a-922f-dff541e0b23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For reference - this is the older api\n",
    "# it will not work anylonger if you have recently installed the openai API\n",
    "\n",
    "#import os\n",
    "#import openai\n",
    "\n",
    "#Authenticate  \n",
    "#openai.api_key = api_key  # this is the key    \n",
    "#openai.organization = api_key2  # this is the key\n",
    "\n",
    "#test\n",
    "#q = \"What is the captial city of Spain?\"\n",
    "#ans = openai.ChatCompletion.create(\n",
    "#  model=\"gpt-3.5-turbo\",\n",
    "#  messages=[\n",
    "#        {\"role\": \"user\", \"content\": q}\n",
    "#    ]\n",
    "#)\n",
    "#print(ans['choices'][0]['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2cdb0a-caed-463a-93da-dd0b74b0a65a",
   "metadata": {},
   "source": [
    "## We can use ChatGPT from code and get an answer to any question. \n",
    "#### This can be:\n",
    "#### * a question from a customer (like an agent)\n",
    "#### * a question for your own work\n",
    "#### * a service that is needed for your code... or more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc2a181b-6cff-4407-9989-a846bca1d25b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obama's last name is Obama.\n"
     ]
    }
   ],
   "source": [
    "#client = OpenAI()\n",
    "# This is an example of a chat completion\n",
    "# We can use it to answer a question\n",
    "\n",
    "response = myclient.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is Obama's last name?\"},\n",
    "  ]\n",
    ")\n",
    "print(response.choices[0].message.content) # this is the answer from GPT-3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "223a76f6-8f57-41f5-a56e-d01764bf1abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'openai.types.chat.chat_completion_message.ChatCompletionMessage'>\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "# What is the type of variable we are printing?\n",
    "print (type(response.choices[0].message)) \n",
    "print (type(response.choices[0].message.content))\n",
    "# using .content gives us the actual text of the answer (string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e73ef72-6a04-446f-b1c9-19a456bcd216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content=\"Obama's last name is Obama.\", role='assistant', function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89521b8e-45f2-4b61-9b96-e968d426e5d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'openai.types.chat.chat_completion.ChatCompletion'>\n",
      "ChatCompletion(id='chatcmpl-8gUcRgmOBqsfYYgIdBqr8wSWUJMFD', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Obama's last name is Obama.\", role='assistant', function_call=None, tool_calls=None))], created=1705138167, model='gpt-3.5-turbo-0613', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=7, prompt_tokens=24, total_tokens=31))\n"
     ]
    }
   ],
   "source": [
    "print(type(response))\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac4bdbdb-a881-47be-8f53-27ac30ce547d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obama's last name is Obama.\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "642d8e0a-befc-4d6a-8c8c-260fbb79ba01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Toyota Corolla: Reliable compact sedan with good fuel efficiency.\n",
      "2. Ford F-150: Best-selling full-size pickup truck known for durability.\n",
      "3. Honda Civic: Popular compact car known for reliability and performance.\n",
      "4. Chevrolet Silverado: Dependable full-size pickup truck offering power and capability.\n",
      "5. Volkswagen Golf: Versatile hatchback known for its high-quality build and driving dynamics.\n"
     ]
    }
   ],
   "source": [
    "# Let's ask another examle question: \n",
    "question = \"What are the most common cars?  I want a short list of 5 with 10 words per item\"\n",
    "response = myclient.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ]\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "86f4dbc3-30ba-46fc-9c67-bc7c65cb2551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Toyota Corolla: Reliable compact car known for its longevity.\n",
      "2. Ford F-150: Best-selling truck with powerful performance and versatility.\n",
      "3. Honda Civic: Popular compact sedan known for its fuel efficiency.\n",
      "4. Chevrolet Silverado: Durable full-size pickup truck with ample towing capacity.\n",
      "5. Volkswagen Golf: Versatile hatchback offering a balance of performance and comfort.\n",
      "That took  2.7752088329871185  seconds\n"
     ]
    }
   ],
   "source": [
    "# Let's time it so we know how long it takes:\n",
    "import timeit\n",
    "from timeit import default_timer as timer\n",
    "start = timer()\n",
    "\n",
    "question = \"What are the most common cars?  I want a short list of 5 with 10 words per item\"\n",
    "response = myclient.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\", # you can change the model\n",
    "  messages=[\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ]\n",
    ")\n",
    "\n",
    "end = timer()\n",
    "print(response.choices[0].message.content)\n",
    "print('That took ',end - start,' seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c61deece-f777-4090-acef-a93e061a3061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Toyota Camry: A reliable, midsize sedan known for excellent fuel efficiency.\n",
      "2. Ford F-150: America's best-selling vehicle recognized for its towing capacity.\n",
      "3. Honda Civic: Compact car praised for its outstanding longevity and reliability.\n",
      "4. Chevrolet Silverado: Full-size pickup loved for its power and versatility.\n",
      "5. Nissan Rogue: Popular compact SUV with great fuel economy and safety.\n",
      "That took  5.401211716001853  seconds\n"
     ]
    }
   ],
   "source": [
    "# Let's try the same question with another model, ie Chat GPT model 4.0:\n",
    "start = timer()\n",
    "question = \"What are the most common cars?  I want a short list of 5 with 10 words per item\"\n",
    "response = myclient.chat.completions.create(\n",
    "  model=\"gpt-4\",\n",
    "  messages=[\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ]\n",
    ")\n",
    "end = timer()\n",
    "print(response.choices[0].message.content)\n",
    "print('That took ',end - start,' seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fde6aa-6d7e-4e7b-b5b4-a45a970af8f9",
   "metadata": {},
   "source": [
    "## Now we will ask it at our own prompt\n",
    "### This could be on your web site or within a chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "06554afa-250a-49b3-b34d-b1e0ba19a018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Dog\" in Spanish is \"perro\".\n"
     ]
    }
   ],
   "source": [
    "# Lets get an answer?\n",
    "q = input ('what do you want to know: ')\n",
    "answer = myclient.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "        {\"role\": \"user\", \"content\": q}\n",
    "    ]\n",
    ")\n",
    "# try what is light made from?\n",
    "print(answer.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9ac02e15-cc7d-434c-b1bc-db30f50e030f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative\n"
     ]
    }
   ],
   "source": [
    "# Sentiment analysis:\n",
    "statement1 = \"This carpet is never clean\"\n",
    "statement2 = \"You are very nice\"\n",
    "\n",
    "q = \"what is the sentiment of \" + statement2 + \". Anwser in one lower case word.\"\n",
    "answer = myclient.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "        {\"role\": \"user\", \"content\": q}\n",
    "    ]\n",
    ")\n",
    "print(answer.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "742b9298-ad2b-42b5-a9ed-f89f47ce04bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative\n"
     ]
    }
   ],
   "source": [
    "# Sentiment analysis:\n",
    "statement1 = \"This hotel is dirty\"\n",
    "statement2 = \"You are very nice\"\n",
    "instruct = \"You are a sentiment analyzer. \\\n",
    "Answer with the user sentiment with one lowercase word\"\n",
    "\n",
    "q = statement1 \n",
    "answer = myclient.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "        {\"role\": \"system\", \"content\": instruct},\n",
    "        {\"role\": \"user\", \"content\": q}\n",
    "    ]\n",
    ")\n",
    "print(answer.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "995c2104-8624-4a4e-8c5b-c3c8d3327fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En español, se dice: \"Te ves genial\" o \"Luces fantástico/a\".\n"
     ]
    }
   ],
   "source": [
    "# easy to translate languages\n",
    "statement = \"You look great\"\n",
    "q = \"how do you say this in Spanish:\" + statement \n",
    "\n",
    "answer = myclient.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "        {\"role\": \"user\", \"content\": q}\n",
    "    ]\n",
    ")\n",
    "print(answer.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4e65d184-2b4a-4b68-bf05-9be8a4988649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Intro]\n",
      "These niggas prayed on my downfall (They did!)\n",
      "These niggas prayed on my downfall\n",
      "On all ten, bitch, I stood tall (You dig)\n",
      "Face shots, I stood tall (Turn up, turn up)\n",
      "\n",
      "[Chorus]\n",
      "I got out of jail and went right to the booth (Yeah)\n",
      "I'm in the streets, bitch, I ain't doing no truce (Turn up, turn up)\n",
      "First day out, I poured lean in my juice (I did)\n",
      "Bitch, I'm a dog, give me a roof (Give me a roof)\n",
      "\n",
      "[Verse 1]\n",
      "So much problems and pain, I don't know what to do (I don't)\n",
      "I think I'm John Gotti, I just don't hang with the mob (Turn up, turn up)\n",
      "I pulled a lot of capers and we set a lot of traps (Brrah, brrah)\n",
      "This rap money slow, so I started selling crack (We got it)\n",
      "I just count a whole M, I'm 'bout to buy me a Jag (Buy me a Jag)\n",
      "I just counted up a hundred bands, nigga, in cash (No cap)\n",
      "And I got hit up in my leg, it burned me for the bag (I got shot)\n",
      "Niggas thought I lost my touch, I just took a lil' break (I took a break)\n",
      "Forgot I'm still the nigga with the Drake (Turn up, turn up)\n",
      "Took a jet to E.M., I'm in the Bay with a jay (Oakland)\n",
      "Ridin' strapped, pray a nigga trip today (Pray a nigga trip today)\n",
      "I'm like fuck the rap, I'm a wrap my Glock (Fuck these niggas)\n",
      "Just tried to pull my pants down in front of the cops (Fucked up)\n",
      "I don't trust nobody, so I keep a Glock (No trust)\n",
      "I remember being broke, now my wrist is on rocks (Wrist be rockin')\n",
      "I been trappin' out three different spots (Trappin', skrrt)\n",
      "All in the trenches, I don't need a bodyguard (No, no)\n",
      "Bitch tried to get with me but I already sold her bones (Already hit)\n",
      "Baggin' up my jeans, got Dickie's on (Baggin' up)\n",
      "Put blood on these jeans, tryna make a bone (Put blood on these)\n",
      "\n",
      "[Chorus]\n",
      "I got out of jail and went right to the booth (Yeah)\n",
      "I'm in the streets, bitch, I ain't doing no truce (Turn up, turn up)\n",
      "First day out, I poured lean in my juice (I did)\n",
      "Bitch, I'm a dog, give me a roof (Give me a roof)\n",
      "\n",
      "[Verse 2]\n",
      "Four deep in that two-seater with them choppas in the back (With them choppas)\n",
      "I'd rather die before a nigga take mine (Yeah)\n",
      "You can have a whole life, I just want lil' time (Just a little, little)\n",
      "I ain't want another dog, I just want lil' mine (Just a little, little)\n",
      "But if a nigga play with me, he ain't gon' see the sunshine (No, no)\n",
      "I don't got no time for bitches, I'm too stuck on grindin' (Bitches, no)\n",
      "In the kitchen whippin' up, I think I'm Fresh Montana (Whip it up)\n",
      "Can't forget the jail calls from my brother, lil' Uzzy (My brother, Uzzy)\n",
      "Hold it down if I catch a case, he can't run from me (My brother, Uzzy)\n",
      "When I caught that case, them niggas turned their backs on me (Brrah, brrah, brrah)\n",
      "But them shows on the road still packed up all the racks for me (Yeah, yeah, yeah)\n",
      "Niggas playin' both sides, iffy, gotta watch for real (No trust)\n",
      "Take my soul, blew through his brains, brain spilled (Blew his brains out)\n",
      "You ain't even gotta sign, I'ma make them drill (Yeah, yeah)\n",
      "'Til the Lord come snatch me, I'ma serve that pack still (Turn up, turn up)\n",
      "\n",
      "[Chorus]\n",
      "I got out of jail and went right to the booth (Yeah)\n",
      "I'm in the streets, bitch, I ain't doing no truce (Turn up, turn up)\n",
      "First day out, I poured lean in my juice (I did)\n",
      "Bitch, I'm a dog, give me a roof (Give me a roof)\n"
     ]
    }
   ],
   "source": [
    "q = \"what are the lyrics of T-grizzlys song 'First day out'?\"\n",
    "answer = myclient.chat.completions.create(\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  messages=[\n",
    "        {\"role\": \"user\", \"content\": q}\n",
    "    ]\n",
    ")\n",
    "tagline = answer.choices[0].message.content\n",
    "print(tagline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1387eb81-ce05-4a55-bc91-da65b3ce4b1c",
   "metadata": {},
   "source": [
    "## Lets create / generate a picture - inside our code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ee17d43e-f91d-4f66-a70d-446ed4d4437a",
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'code': 'content_policy_violation', 'message': 'Your request was rejected as a result of our safety system. Your prompt may contain text that is not allowed by our safety system.', 'param': None, 'type': 'invalid_request_error'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Use the tagline from the last example.  Also can use dall-e-3 model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m pic \u001b[38;5;241m=\u001b[39m \u001b[43mmyclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m  \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdall-e-3\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m  \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtagline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m  \u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m  \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m1024x1024\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m image_url \u001b[38;5;241m=\u001b[39m pic\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39murl\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(image_url)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/resources/images.py:249\u001b[0m, in \u001b[0;36mImages.generate\u001b[0;34m(self, prompt, model, n, quality, response_format, size, style, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate\u001b[39m(\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    207\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    208\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ImagesResponse:\n\u001b[1;32m    209\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;124;03m    Creates an image given a prompt.\u001b[39;00m\n\u001b[1;32m    211\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;124;03m      timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 249\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/images/generations\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprompt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquality\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mquality\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstyle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstyle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m            \u001b[49m\u001b[43mimage_generate_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mImageGenerateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mImagesResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/_base_client.py:1112\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1099\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1100\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1107\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1108\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1109\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1110\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1111\u001b[0m     )\n\u001b[0;32m-> 1112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/_base_client.py:859\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    850\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    851\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    852\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    857\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    858\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/openai/_base_client.py:949\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    946\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m    948\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 949\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    951\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m    952\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m    953\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    956\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m    957\u001b[0m )\n",
      "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'code': 'content_policy_violation', 'message': 'Your request was rejected as a result of our safety system. Your prompt may contain text that is not allowed by our safety system.', 'param': None, 'type': 'invalid_request_error'}}"
     ]
    }
   ],
   "source": [
    "# Use the tagline from the last example.  Also can use dall-e-3 model\n",
    "pic = myclient.images.generate(\n",
    "  model=\"dall-e-3\",\n",
    "  prompt=tagline,\n",
    "  n=1,\n",
    "  size=\"1024x1024\"\n",
    ")\n",
    "image_url = pic.data[0].url\n",
    "print(image_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da66889-8c81-405d-8716-076419dae707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example from OpenAI documentation\n",
    "# https://platform.openai.com/docs/guides/images/usage\n",
    "pic = myclient.images.generate(\n",
    "  model=\"dall-e-3\",\n",
    "  prompt=\"A cute baby sea otter\",\n",
    "  n=1,\n",
    "  size=\"1024x1024\"\n",
    ")\n",
    "image_url = pic.data[0].url\n",
    "print(image_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792f6d21-7d11-45fe-8b4d-0ea1a5079c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# there is more information in the pic object\n",
    "print(type(pic))\n",
    "print(pic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d723eb-ad77-466c-9fb1-fa3b7059d0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (pic.created)\n",
    "print()\n",
    "print(pic.data[0].revised_prompt)\n",
    "print()\n",
    "print(pic.data[0].url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23ac1f5-84e8-4b6d-8475-b2f6d860324e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate a picture on demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ecec734-f973-49cd-a084-e44904da766a",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = input('What picture do you want to generate') \n",
    "\n",
    "pic = myclient.images.generate(\n",
    "  model=\"dall-e-3\",\n",
    "  prompt=p,\n",
    "  n=1,\n",
    "  size=\"1024x1024\"\n",
    ")\n",
    "image_url = pic.data[0].url\n",
    "print(image_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a00f827-66b0-4b46-8843-20c531416e7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4345c183-05af-498c-8a74-9e12f069cc30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content='This image depicts a famous painting known as \"Girl with a Pearl Earring.\" It showcases the head of a young woman gazing over her shoulder, featuring a striking blue and yellow headscarf and a large pearl earring. The subject\\'s expression is enigmatic, contributing to the artwork\\'s mystique. With its rich colors, delicate skin tones, and strong contrast between the figure and the dark background, the painting is widely admired for its compositional simplicity and powerful visual impact.', role='assistant', function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "# vision - this will enable chatgpt to tell you about a picture\n",
    "# learn more here: https://platform.openai.com/docs/guides/vision\n",
    "\n",
    "response = myclient.chat.completions.create(\n",
    "  model=\"gpt-4-vision-preview\",\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "        {\"type\": \"text\", \"text\": \"What’s in this image?\"},\n",
    "        {\n",
    "          \"type\": \"image_url\",\n",
    "          \"image_url\": {\n",
    "            \"url\": \"https://www.singulart.com/blog/wp-content/uploads/2023/10/Famous-Portrait-Paintings-848x530-1.jpg\",\n",
    "          },\n",
    "        },\n",
    "      ],\n",
    "    }\n",
    "  ],\n",
    "  max_tokens=300,\n",
    ")\n",
    "print(response.choices[0].message)\n",
    "\n",
    "# or try \n",
    "# \"url\": \"https://www.singulart.com/blog/wp-content/uploads/2023/10/Famous-Portrait-Paintings-848x530-1.jpg\"\n",
    "# \"url\": \"https://images.hellomagazine.com/horizon/landscape/86955af8d14a-landmarks-worldwide-t.jpg\"\n",
    "# \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "be34e9aa-9a82-4587-9698-01ba4886f92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text to speech\n",
    "# https://platform.openai.com/docs/guides/text-to-speech\n",
    "# this will create the file speech.mp3 from the text below with a voice called alloy\n",
    "\n",
    "from pathlib import Path\n",
    "speech_file_path = \"./speech.mp3\"    # Path(__file__).parent / \"speech.mp3\"\n",
    "response = myclient.audio.speech.create(\n",
    "  model=\"tts-1\",\n",
    "  voice=\"alloy\",\n",
    "  input=\"Today is a wonderful day to build something people love!\"\n",
    ")\n",
    "\n",
    "response.stream_to_file(speech_file_path)\n",
    "\n",
    "# Experiment with different voices (alloy, echo, fable, onyx, nova, and shimmer) \n",
    "# to find one that matches your desired tone and audience. The current voices are optimized for English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c06a4b-329b-4711-9a82-e8fa3d74b9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# speech to text\n",
    "# this will take that voice file and turn it back to text\n",
    "\n",
    "#audio_file= open(\"/path/to/file/audio.mp3\", \"rb\")\n",
    "audio_file = open(\"speech.mp3\", \"rb\")\n",
    "transcript = myclient.audio.transcriptions.create(\n",
    "  model=\"whisper-1\", \n",
    "  file=audio_file\n",
    ")\n",
    "print(transcript.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c0db37-1b03-4bd5-b82e-b1d884bfc7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# translate language from audio file\n",
    "# this will transale a German audio file to English text\n",
    "\n",
    "audio_file= open(\"german.mp3\", \"rb\")\n",
    "transcript = myclient.audio.translations.create(\n",
    "  model=\"whisper-1\", \n",
    "  file=audio_file\n",
    ")\n",
    "print(transcript.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
